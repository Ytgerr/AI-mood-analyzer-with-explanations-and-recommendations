{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "from captum.attr import IntegratedGradients\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797bed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "model.eval()\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ce23fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I do not want to do it!\n",
      "Predicted label: negative\n",
      "Confidence: 0.9490\n",
      "\n",
      "Top 3 tokens with the strongest attributions:\n",
      "I                    | Attribution: 0.3822\n",
      "not                  | Attribution: 0.9909\n",
      "want                 | Attribution: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_mapping = {\n",
    "    \"LABEL_0\": \"negative\",\n",
    "    \"LABEL_1\": \"neutral\",\n",
    "    \"LABEL_2\": \"positive\"\n",
    "}\n",
    "\n",
    "text = \"I do not want to do it!\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_ids = inputs['input_ids'].long()  \n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "\n",
    "result = sentiment_pipeline(text)\n",
    "label = result[0]['label']\n",
    "mapped_label = label_mapping[label]\n",
    "confidence = result[0]['score']\n",
    "\n",
    "\n",
    "label_to_index = {\"LABEL_0\": 0, \"LABEL_1\": 1, \"LABEL_2\": 2}\n",
    "target = label_to_index[label]\n",
    "\n",
    "embedding_layer = model.roberta.embeddings  \n",
    "with torch.no_grad():\n",
    "    embeddings = embedding_layer(input_ids).clone().detach().requires_grad_(True)\n",
    "\n",
    "def forward_func(embeddings, attention_mask):\n",
    "    model_inputs = {\n",
    "        'inputs_embeds': embeddings,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    outputs = model(**model_inputs)\n",
    "    return outputs.logits\n",
    "\n",
    "ig = IntegratedGradients(forward_func)\n",
    "\n",
    "attributions, delta = ig.attribute(\n",
    "    inputs=embeddings,\n",
    "    additional_forward_args=(attention_mask,),\n",
    "    target=target,\n",
    "    return_convergence_delta=True\n",
    ")\n",
    "\n",
    "attributions_sum = attributions.sum(dim=-1).squeeze(0)\n",
    "attributions_sum = attributions_sum.detach().numpy()\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
    "tokens = [token.replace(\"Ä \", \"\") for token in tokens]\n",
    "\n",
    "attributions_sum = attributions_sum / np.max(np.abs(attributions_sum))\n",
    "top_indices = np.argsort(np.abs(attributions_sum))[-3:]  \n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Predicted label: {mapped_label}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "\n",
    "print(\"\\nTop 3 tokens with the strongest attributions:\")\n",
    "\n",
    "for idx in top_indices:\n",
    "    token = tokens[idx]\n",
    "    \n",
    "    attr = attributions_sum[idx]\n",
    "    print(f\"{token:20} | Attribution: {attr:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
